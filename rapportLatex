% !TEX program = xelatex
\documentclass[12pt,a4paper]{report}

% --- Preamble ---
\usepackage{fontspec}
\usepackage{polyglossia}

% --- Scrum visuals & tables ---
\usepackage{float}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

\setmainlanguage{english}
\setotherlanguage{arabic}
\setmainfont{Times New Roman}
\newfontfamily\arabicfont[Script=Arabic]{Amiri}

\usepackage{geometry}
\geometry{margin=1in}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage{caption}
\captionsetup{style=default}
\usepackage[hidelinks]{hyperref}
\usepackage[nameinlink]{cleveref}
\usepackage{appendix}
\usepackage{listings}
\lstset{
  basicstyle=\small\ttfamily,
  breaklines=true,
  frame=single,
  showstringspaces=false,
  language=rust
}
\usepackage{minitoc}

% ---------------- Document ----------------
\begin{document}

\graphicspath{{figures/}}

% Roman numbering for front matter
\pagenumbering{roman}

% ---- ToC ----
\tableofcontents
\addcontentsline{toc}{chapter}{\listfigurename}
\listoffigures
\addcontentsline{toc}{chapter}{\listtablename}
\listoftables
\newpage

% Switch to arabic numbering for main matter
\pagenumbering{arabic}

% ================================
% General Introduction
% ================================
\chapter*{General Introduction}
\addcontentsline{toc}{chapter}{General Introduction}

Effective identity resolution in Arabic remains constrained by orthographic variability, phonetic ambiguity, and inconsistent data-entry practices across decades of records. This thesis, conducted with the Ministère de l'Intérieur, presents a pragmatic system that delivers high-accuracy matching at operational speed. The approach is hybrid: deterministic normalization unifies script variants and removes diacritics; a custom phonetic encoder (Aramix Soundex) captures sound-level proximity specific to Arabic; and weighted fuzzy similarity (Jaro–Winkler and Levenshtein) aggregates evidence across biographical fields under a transparent scoring model. The implementation prioritizes performance and maintainability. A Rust backend exposes a REST API and executes parallelized scoring via Rayon; an Angular SPA supports agents with responsive workflows; PostgreSQL holds normalized identity records; and Neo4j models kinship, enabling relationship-aware exploration. A separate n8n pipeline orchestrates family-tree visualization, invoking Gemini for schema simplification and a Python renderer for diagram generation. The architecture is containerized for reproducibility and deployment simplicity. Beyond technical accuracy, the system targets measurable administrative value: faster case handling, consistent decisions, auditable outcomes, and reduced manual triage. The document proceeds from context and requirements to the algorithmic foundations, system design, and evaluation. Results show that the combination of targeted normalization, Arabic-specific phonetics, and disciplined scoring yields robust real-time matching suitable for production use.

% ================================
% Chapter 1
% ================================
\chapter{Presentation of the Host Organization}

\section*{Introduction}
This chapter presents the professional environment in which the internship took place. It begins with an overview of Inetum as a global IT leader, describing its history, structure, and worldwide presence. Next, it focuses on the Tunisian subsidiary—its organization, strategic areas of activity, and contribution to the group's overall mission. Finally, it discusses the company's core values, main clients, and key technology partners. By understanding the organizational framework, this chapter establishes the professional and cultural context that shaped the development of the project described in the following sections.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.6\textwidth]{figures/inetum.jpg}
\caption{Inetum, the host organization.}
\label{fig:inetum-intro}
\end{figure}

\section{Inetum: Global Overview}
Inetum, formerly known as GFI (Groupe Français d'Informatique), is a global IT services company specializing in digital transformation and consulting. In 2021, the group rebranded to Inetum to mark a new era as an expert in the "digital flow." Today, Inetum operates in over 27 countries across Europe, the Americas, Asia, and Africa. With a dedicated team of nearly 27,000 professionals, the company generated a revenue of €2.2 billion in the last fiscal year. It provides a wide range of services—from consulting and innovation to application development—leveraging its local expertise on a global scale.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/Key_figures_of_Inetum.png}
\caption{Key figures of Inetum (employees, offices, revenue, etc.).}
\label{fig:key-figures-inetum}
\end{figure}

\section{Inetum in the World}
Inetum maintains strong alliances with leading global software and hardware vendors to deliver robust and integrated solutions. Strategic partners include SAP, Microsoft, IBM, Oracle, Salesforce, and Sage. These collaborations enable Inetum to provide its clients with cutting-edge enterprise technologies and a broad portfolio of digital services.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/Inetum_Worldwide.png}
\caption{Inetum worldwide presence.}
\label{fig:inetum-worldwide}
\end{figure}

\section{Inetum in Tunisia}
The Tunisian subsidiary of Inetum has over a decade of experience in IT organization and systems integration. With more than 250 employees and annual revenues exceeding €10 million, Inetum Tunisia combines global expertise with local agility. The branch operates in four main areas:

\begin{itemize}
\item \textbf{Consulting and Services:} Custom development for web and mobile portals, IoT solutions, and UX design.
\item \textbf{Business Intelligence:} Advanced analytics and data-driven decision support.
\item \textbf{Enterprise Solutions:} Implementation and management of ERP systems such as SAP and Microsoft Dynamics.
\item \textbf{Human Resources (SIRH):} Deployment of HR information systems tailored to organizational needs.
\end{itemize}

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/org_chart.png}
\caption{High-level organizational chart (Inetum Tunisia).}
\label{fig:org-chart}
\end{figure}

\section{Core Values}
Inetum's culture is built around five fundamental values: \textbf{ambition}, \textbf{commitment}, \textbf{excellence}, \textbf{innovation}, and \textbf{solidarity}. These principles define the company's identity, guide decision-making, and ensure that every client engagement reflects both technical quality and social responsibility.

\section{Clients and Strategic Partners}
Inetum serves clients from diverse sectors, including:

\begin{itemize}
\item Banking and Financial Services
\item Telecommunications and Technology
\item Retail and Services
\item Government and Public Sector
\item Manufacturing and Industry
\end{itemize}

\begin{figure}[htbp]
\centering
\includegraphics[width=.9\textwidth]{figures/Clients.jpg}
\caption{Representative Inetum clients across sectors.}
\label{fig:inetum-clients}
\end{figure}

Inetum also relies on long-term partnerships with major global technology providers—SAP, Microsoft, IBM, Oracle, Salesforce, and Sage—ensuring access to the most reliable and innovative enterprise solutions.

\begin{figure}[htbp]
\centering
\includegraphics[width=.9\textwidth]{figures/partners.png}
\caption{Strategic partners supporting Inetum's services.}
\label{fig:inetum-partners}
\end{figure}

\section{Conclusion}
This chapter provided an in-depth overview of Inetum, its global reach, and its Tunisian subsidiary's role in the company's broader mission. Understanding the host organization's structure, culture, and strategic objectives is essential, as they directly influence the expectations, methodologies, and professional standards applied throughout this project. Inetum's commitment to innovation and digital transformation defines a professional context where quality, agility, and client satisfaction are paramount. Working within such an ecosystem provided not only technical exposure but also insight into how large-scale IT organizations structure their processes, manage client relationships, and sustain operational excellence across continents. The next chapter will build upon this foundation by introducing the context of the internship itself—its objectives, the problem it addresses, and the overall approach adopted to design and implement an intelligent identity-matching system aligned with Inetum's culture of innovation.

% ================================
% Chapter 2
% ================================
\chapter{Project Context and Planning}

\section{Introduction}
This chapter transitions from the organizational context to the core of the project itself. The primary goal of this initiative is to replace an outdated, manual, and often inefficient identity verification system with a modern, high-performance, and intelligent solution. The existing processes, heavily reliant on paper-based records and basic digital tools, are inadequate for handling the linguistic nuances of Arabic names, leading to significant delays and potential errors that impact citizen services. This chapter will provide a comprehensive overview of the project, beginning with a detailed problem statement that highlights the specific challenges faced by the Ministère de l'Intérieur. From there, we will define the project's objectives and scope, outlining what we aim to achieve. We will then present the proposed solution, including its high-level architecture and the technology stack chosen to meet the demanding requirements of performance, accuracy, and scalability. Finally, the chapter will cover the development methodologies and project planning that guided the execution of this complex undertaking, ensuring a structured path from conception to deployment.

\section{Problem Statement}
The motivation for this project stems from the shortcomings of the current systems, which are ill-equipped to handle the complexities of Arabic names. The core challenges can be summarized as follows:

\begin{itemize}
\item \textbf{Linguistic Variability of Arabic Names:} Orthographic and phonetic ambiguities create significant challenges for digital systems that rely on exact matches.
\item \textbf{Costs and Risks of Manual Verification:} The manual process is slow, expensive, and prone to errors that can deny citizens access to essential services.
\item \textbf{Need for a Modern and Responsive UX:} The current tools are outdated and inefficient, leading to a poor user experience for government agents and long wait times for citizens.
\end{itemize}

The problem is particularly acute for individuals who may lack a national ID number, including:

\begin{itemize}
\item \textbf{Newborns and Unregistered Births:} Especially in rural areas where births may not be officially declared in a timely manner.
\item \textbf{Minors:} Who often rely on family booklets or birth certificates before being issued a national ID at age 16 or 18.
\item \textbf{Citizens Living Abroad:} Who may have lost documents or only have foreign transliterations of their names.
\item \textbf{Elderly People:} Many of whom were born before the era of digitization and are only listed in handwritten registers.
\item \textbf{Vulnerable Populations:} Such as homeless individuals or refugees who may be disconnected from the civil registry.
\end{itemize}

\section{Study of the Existing System}
The current identity verification process is predominantly manual, leading to significant inefficiencies and risks. Key characteristics of the existing system include:

\begin{itemize}
\item \textbf{Manual Paper-Based Management:} All requests and supporting documents are archived on printed media. Biographical information is often entered by hand into physical registers or basic spreadsheets.
\item \textbf{Visual Matching and Spreadsheets:} Agents manually scan lists to identify duplicates and inconsistencies. This process relies heavily on intensive filtering and sorting in tools like Excel or Access, which lack sophisticated similarity algorithms.
\item \textbf{Lack of Centralization and Automation:} Data is fragmented across silos in different departments, consulates, and local databases. The process is slow, prone to human error, and provides no real-time feedback.
\end{itemize}

\section{Objectives \& Success Criteria}
The primary objective of this project is to develop a highly accurate and efficient system for Arabic identity matching. Success will be measured by the following criteria:

\begin{itemize}
\item \textbf{Accuracy:} The system must achieve a high precision and recall rate in matching Arabic names.
\item \textbf{Performance:} The matching process should be fast enough to handle real-time requests.
\item \textbf{Scalability:} The system should be able to handle a large volume of data and users.
\item \textbf{Usability:} The user interface should be intuitive and easy to use.
\end{itemize}

\section{Project Scope}

\subsection{Scope, Dataset, and Boundaries}
\begin{itemize}
\item The project aims to develop a system capable of matching Arabic identities from a structured dataset.
\item The dataset contains numerous identity records, each including:
  \begin{itemize}
  \item Full name
  \item Date of birth
  \item Place of birth
  \end{itemize}
\item The system assumes:
  \begin{itemize}
  \item Input data follows a predefined format.
  \item The database is accessible and properly structured.
  \end{itemize}
\item The primary constraint is the \textbf{performance} of the matching algorithm, which must ensure real-time response capability.
\item The solution must therefore balance accuracy with computational efficiency to maintain scalability under high-load conditions.
\end{itemize}

\subsection{Stakeholders \& Core Use Cases}
The main stakeholders of the project are the users who need to match identities (e.g., government agents), and the administrators who manage the system. The core use cases include:

\begin{itemize}
\item Searching for a citizen's record using partial or inexact information.
\item Verifying an individual's identity when a national ID is not available.
\item Reconstructing family relationships through the family tree visualization.
\end{itemize}

\begin{figure}[htbp]
\centering
\includegraphics[width=.9\textwidth]{figures/Core Use Cases Hierarchy.png}
\caption{Core Use Cases Hierarchy.}
\label{fig:Core_Us_Cases_Hierarchy}
\end{figure}

\section{Proposed Solution}

\subsection{Proposed Solution-مطابق الهوية}
The proposed solution is a modern, hybrid system that addresses the core challenges of Arabic name matching. Hybrid identity matching combines text normalization to unify orthographic variants, a custom phonetic algorithm (Aramix Soundex), and fuzzy matching using Levenshtein and Jaro-Winkler. Visualization and automation are achieved through n8n for process orchestration, Gemini AI for data parsing and simplification, and Python for automated diagram generation. The technology stack includes Angular for the frontend, Rust with Axum for a high-performance backend, PostgreSQL for structured identity records, and Neo4j for managing family tree relationships. For deployment, the entire system is containerized.

\subsection{General Overview}
This project focuses on the development of an identity matching system specifically adapted to the Arabic language. Unlike Latin-based systems, Arabic introduces unique challenges such as orthographic variability, phonetic ambiguity, and transliteration issues. For example, the system must handle:

\begin{itemize}
\item \textarabic{همزة} (ء) in its multiple forms (\textarabic{أ، إ، ؤ، ئ}) and their interchangeable usage.
\item \textarabic{تنوين} (\textarabic{ً ٍ ٌ}) which may appear or be omitted in different records.
\item Diacritics (\textarabic{حركات}) like \textarabic{َ ِ ُ} that are often inconsistently written.
\item Ligatures such as (\textarabic{ﻻ}) for "lam-alef".
\item Variations in letter forms, e.g., "\textarabic{ة}" vs "\textarabic{ه}", or "\textarabic{ى}" vs "\textarabic{ي}".
\item Prefix and naming conventions (\textarabic{الـ، ابن، بن، بنت، أبو، أم}).
\end{itemize}

The objective is to design a reliable and scalable solution that integrates seamlessly into existing organizational workflows while accounting for these Arabic-specific linguistic variations. By combining normalization, phonetic encoding (Aramix Soundex), and fuzzy similarity scoring, the system ensures robust identity matching across heterogeneous data sources.

\subsection{High-Level Architecture}
The system is designed with a microservices-oriented approach, separating the core matching logic from the family tree generation process. The main components are:

\begin{itemize}
\item \textbf{Angular Frontend:} A responsive single-page application providing the user interface for government agents.
\item \textbf{Rust Backend:} A high-performance REST API that handles user authentication, identity matching requests against the PostgreSQL database, and initiates family tree generation by calling the n8n workflow.
\item \textbf{Databases:} PostgreSQL for storing primary identity records and Neo4j for modeling and querying complex family relationships as a graph.
\item \textbf{n8n Automation Workflow:} A dedicated workflow that orchestrates the entire family tree visualization process, acting as a middleware that connects multiple services.
\end{itemize}

The family tree generation follows a specific, automated sequence, as summarized in the following speaker notes.

\subsubsection*{Family Tree Diagram Speaker Notes (short)}
\begin{itemize}
\item The process begins when an agent requests the generation of an individual's family tree.
\item The request is sent to the \textbf{Family Tree API}, which verifies user credentials.
\item The API retrieves raw family relationship data and forwards it to the \textbf{n8n workflow service}.
\item The AI component processes the data, converting it into structured JSON suitable for visualization.
\item The structured output is returned to the frontend as a tree representation.
\item The user interface then renders the complete interactive family tree for the agent to navigate.
\end{itemize}

\subsubsection*{n8n Workflow (Concise Script)}
\begin{itemize}
\item The workflow begins when the \textbf{Webhook} node receives raw JSON data from Neo4j containing relationship information.
\item The payload is forwarded to \textbf{Google Gemini} with a prompt to parse and normalize entities and links.
\item Gemini returns a structured output that is passed to a \textbf{Python script} responsible for rendering family tree diagrams.
\item The generated visuals are then sent back to the frontend through the webhook response.
\end{itemize}

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/n8nwork.png}
\caption{n8n workflow for family tree generation.}
\label{fig:n8n-workflow}
\end{figure}

\paragraph{LLM Choice}
Currently, Gemini serves as the AI model within the n8n workflow. For future iterations, a self-hosted LLM (such as Ollama or another open-source alternative) could be dedicated to this function to provide a more secure and faster response.

\subsection{Technology Stack}
The choice of technologies was driven by the project's requirements for performance, scalability, and modern user experience. Each component was selected to fill a specific role in the architecture.

\begin{itemize}
\item \begin{center}
\includegraphics[height=3.2em]{figures/Rust.png}
\textbf{Rust:} Chosen for the backend due to its exceptional performance, memory safety, and concurrency features, which are critical for handling real-time matching requests efficiently.
\end{center}

\item \begin{center}
\includegraphics[height=3.2em]{figures/Angular.png}
\textbf{Angular:} A powerful frontend framework used to build the responsive and feature-rich single-page application (SPA) that serves as the user interface for government agents.
\end{center}

\item \begin{center}
\includegraphics[height=4.2em]{figures/postgres.png}
\textbf{PostgreSQL:} A reliable and scalable open-source relational database used to store the core identity records, offering robust data integrity and performance.
\end{center}

\item \begin{center}
\includegraphics[height=3.2em]{figures/Neo4j.png}
\textbf{Neo4j:} A native graph database ideal for managing and querying the complex, interconnected data of family trees. Its graph-based model allows for efficient traversal of familial relationships.
\end{center}

\item \begin{center}
\includegraphics[height=3.2em]{figures/n8n.png}
\textbf{n8n:} A low-code workflow automation tool used to orchestrate the family tree visualization process. It acts as the glue between the backend, the AI model, the Python script, and Google Drive, allowing for complex process automation without cluttering the main backend codebase.
\end{center}

\item \begin{center}
\includegraphics[height=3.2em]{figures/Gemini.png}
\textbf{Google Gemini API:} A powerful generative AI model used within the n8n workflow to intelligently parse the complex JSON output from Neo4j and transform it into a simplified, structured format suitable for the diagram generation script.
\end{center}

\item \begin{center}
\includegraphics[height=3.2em]{figures/python.png}
\textbf{Python:} Used for a dedicated microservice whose sole responsibility is to take the simplified JSON data and generate a family tree diagram. This separation of concerns keeps the main backend focused on its core tasks.
\end{center}

\item \begin{center}
\includegraphics[height=3.2em]{figures/docker.png}
\textbf{Docker:} Used for containerizing the entire application stack (backend, frontend, databases), ensuring consistent development and deployment environments.
\end{center}
\end{itemize}

\subsection{Deployment Strategy}
The entire system is designed for containerized deployment using Docker, ensuring consistency across development, testing, and production environments. This approach simplifies dependency management and enhances portability.

\subsubsection{Application Containerization}
\begin{itemize}
\item Both the Rust backend and the Angular frontend are containerized using multi-stage \textbf{Dockerfiles}.
\item The first stage (\textit{builder}) includes the full SDKs (Rust or Node.js) to compile the application and its dependencies.
\item The second stage copies only the compiled artifacts — the Rust binary and Angular static files — into lightweight production images such as \texttt{debian:bullseye-slim} or \texttt{nginx:alpine}.
\item This approach produces smaller images, minimizes the attack surface, and excludes unnecessary build tools and source code.
\item The result is a more secure, efficient, and production-ready deployment.
\end{itemize}

\subsubsection{Service Orchestration}
\begin{itemize}
\item All services are orchestrated using \textbf{Docker Compose}, defined in the \texttt{docker-compose.yml} file.
\item The file specifies services, networks, and volumes, automating the full-stack startup process.
\item The main components include:
  \begin{itemize}
  \item \textbf{Angular Frontend:} Built from its Dockerfile and served via Nginx.
  \item \textbf{Rust Backend:} Built from its dedicated Dockerfile.
  \item \textbf{PostgreSQL:} Provides the relational database service.
  \end{itemize}
\item Docker networking ensures that all services communicate seamlessly, while volumes persist database state across container restarts.
\item The \textbf{n8n workflow}, responsible for family tree generation, runs as a separate deployment.
\item It can be self-hosted using the official Docker image and integrates with the main application through secure webhook calls, following microservice architecture best practices.
\end{itemize}

\section{Development Methodology}
The project follows a hybrid approach, combining the Scrum framework for project management with the CRISP-DM methodology for the data science lifecycle. Scrum governed how the team organised work, inspected progress, and adapted priorities, while CRISP-DM supplied the analytical structure for the data-oriented tasks tackled during each sprint.

\subsection{Scrum Framework}

\subsubsection{Agile and Scrum Overview}
Scrum is an \textbf{agile framework} designed to manage complex projects through short, iterative cycles called \textit{sprints}. It emphasizes \textbf{collaboration, adaptability, and continuous improvement}. The framework enables teams to deliver incremental value through frequent inspection, feedback, and adaptation. This approach was selected to ensure flexibility and maintain a steady delivery rhythm while meeting evolving project needs.

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{figures/scrum.jpg}
\caption{Scrum framework adopted for the project.}
\end{figure}

\subsubsection*{Team Roles and Responsibilities}
\begin{itemize}
\item The Scrum Team was intentionally kept small to ensure rapid feedback and clear communication.
\item \textbf{Product Owner:} Representative from the Ministère de l'Intérieur responsible for defining needs, validating increments, and refining acceptance criteria.
\item \textbf{Scrum Master:} Mouaïa Ben Hamed, responsible for facilitating Scrum ceremonies, resolving blockers (such as secure dataset access), and upholding Scrum principles.
\item \textbf{Developer:} Ilef (author), responsible for implementing increments, estimating workload, preparing demos, and documenting improvement actions.
\end{itemize}

\begin{table}[htbp]
\centering
\caption{Scrum Team Roles and Responsibilities}
\begin{tabular}{|l|l|p{8cm}|}
\hline
\textbf{Role} & \textbf{Member} & \textbf{Main Responsibility} \\
\hline
Product Owner & Representative from the Ministère de l'Intérieur & Defines needs, validates increments, and refines acceptance criteria. \\
\hline
Scrum Master & Mouaïa Ben Hamed & Facilitates Scrum ceremonies, removes blockers, and ensures adherence to Scrum principles. \\
\hline
Developer & Ilef (Author) & Implements increments, estimates workload, prepares demos, and documents improvement actions. \\
\hline
\end{tabular}
\label{tab:scrum-team}
\end{table}

\subsubsection*{Sprint Cadence and Planning}
\begin{itemize}
\item Development was organized into four sprints, each lasting two weeks.
\item Each sprint began with a \textbf{Sprint Planning} session to define the Sprint Goal and select high-value Product Backlog items.
\item Tasks were decomposed and sized according to team capacity, accounting for academic constraints such as exams and teaching duties.
\item Sprint Goals prioritized vertical slices, ensuring each iteration delivered a deployable increment in the staging environment.
\end{itemize}

\subsubsection{Scrum Events}
Scrum defines four key time-boxed events that ensure communication, transparency, and continuous improvement within the team:

\begin{itemize}
\item \textbf{Sprint Planning:} Conducted at the start of each sprint to define the \textit{Sprint Goal}, select Product Backlog items, and estimate workloads.
\item \textbf{Daily Scrum:} A short 15-minute meeting held each day to synchronize progress, identify blockers, and plan work for the next 24 hours.
\item \textbf{Sprint Review:} Takes place at the end of the sprint to present the completed increment to the Product Owner and collect feedback for future improvements.
\item \textbf{Sprint Retrospective:} Conducted after the review to reflect on the process, identify lessons learned, and define actionable improvements for the next sprint.
\end{itemize}

\subsubsection*{Scrum Ceremonies and Contribution to Progress}
\begin{itemize}
\item \textbf{Daily Scrums} (15 minutes) were used to synchronize progress, identify blockers (e.g., missing Neo4j indexes), and refocus on the Sprint Goal.
\item \textbf{Sprint Reviews} showcased the completed increment to the Product Owner, such as the matching dashboard (Sprint 2) and family tree visualization (Sprint 3).
\item Feedback from Sprint Reviews was immediately integrated into backlog reprioritization.
\item \textbf{Sprint Retrospectives} followed each review to extract lessons learned and define actionable improvements.
\item Examples of retrospective actions included pairing backlog refinement with architecture spikes and rescheduling Daily Scrums for better alignment.
\end{itemize}

\subsubsection*{Scrum Artifacts and Backlog Evolution}
\begin{itemize}
\item The \textbf{Product Backlog} was reviewed and refined weekly.
\item Large epics were decomposed into detailed user stories with explicit acceptance criteria.
\item Stories were prioritized based on business value, risk reduction, and technical feasibility.
\item The \textbf{Sprint Backlog} tracked selected user stories and their subtasks using a Kanban board with columns: \textit{To Do}, \textit{In Progress}, \textit{Review}, and \textit{Done}.
\item This structure ensured workflow transparency during Daily Scrum meetings.
\item Items not meeting the Definition of Done (code merged, tests covering ≥ 80\% of modified lines, documentation updated, demo rehearsed, PO sign-off) were deliberately carried over.
\item Root causes of unfinished items were analyzed during sprint retrospectives to prevent recurrence.
\end{itemize}

\begin{table}[H]
\centering
\small
\begin{tabular}{|l|l|p{8.5cm}|}
\hline
\textbf{Sprint} & \textbf{Goal} & \textbf{Delivered Increment (examples)} \\
\hline
S1 & Backend foundation & Axum skeleton, initial /match endpoint, scoring v1, CI build. \\
\hline
S2 & Accuracy uplift & PostgreSQL integration, normalization pipeline, Aramix Soundex, load test harness. \\
\hline
S3 & Usability & Angular search dashboard, Neo4j graph queries, n8n orchestration, UX enhancements. \\
\hline
S4 & Hardening \& deploy & Golden-set validation tests, performance tuning (Rayon), Docker Compose stack, release documentation. \\
\hline
\end{tabular}
\caption{Sprint summary highlighting increment-driven delivery.}
\label{tab:sprint-summary}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{figures/gant.png}
\caption{Project Gantt Chart showing the timeline from 4 February to 31 July 2025, including all development sprints, reviews, and wrap-up phases.}
\label{fig:gantt-chart}
\end{figure}

\subsection{CRISP-DM Methodology}
The data science component of the project followed the \textbf{Cross-Industry Standard Process for Data Mining (CRISP-DM)}. This methodology provides a structured approach to planning a data mining project. It is a robust and well-proven methodology. The phases were applied as follows:

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{figures/crispdm.jpg}
\caption{CRISP-DM Methodology.}
\label{fig:crisp-dm}
\end{figure}

\begin{itemize}
\item \textbf{Business Understanding:}
  \begin{itemize}
  \item Defined the main challenge of identity matching within the operational context of the Ministère de l'Intérieur.
  \item Clarified project goals and requirements from a business perspective.
  \end{itemize}

\item \textbf{Data Understanding:}
  \begin{itemize}
  \item Analyzed identity records to detect patterns, inconsistencies, and linguistic variations in Arabic names.
  \item Explored and assessed data quality issues during initial collection.
  \end{itemize}

\item \textbf{Data Preparation:}
  \begin{itemize}
  \item Built a text normalization pipeline to clean and standardize names before matching.
  \item Transformed raw identity data into a structured and consistent dataset for modeling.
  \end{itemize}

\item \textbf{Modeling:}
  \begin{itemize}
  \item Designed and implemented a hybrid matching algorithm combining \textbf{Aramix Soundex} with fuzzy string similarity metrics.
  \item Tuned algorithm parameters for optimal precision and recall.
  \end{itemize}

\item \textbf{Evaluation:}
  \begin{itemize}
  \item Assessed model performance using a predefined golden set of matches for accuracy.
  \item Conducted load testing to measure system speed and scalability.
  \item Verified all modeling steps before final deployment.
  \end{itemize}

\item \textbf{Deployment:}
  \begin{itemize}
  \item Containerized both backend and frontend components using Docker.
  \item Deployed the complete system to a production environment for end-user access.
  \end{itemize}
\end{itemize}

By integrating these phases into the Scrum sprints, the development of the matching engine was both systematic and agile, ensuring that the data science components were developed iteratively and aligned with the overall project goals.

\section{Project Planning}

\subsection{Sprint Roadmap \& Timeline}
The six-month academic calendar was translated into a cadence of four production sprints lasting two weeks each, separated by short buffer periods for exams and stakeholder availability. This cadence ensured regular delivery while keeping room for inspection and adaptation.

\noindent Table~\ref{tab:sprint-objectives} summarises the goal, main deliverables, and the most impactful ceremony outcomes for every sprint.

\begin{longtable}{|p{1.3cm}|p{3.7cm}|p{6cm}|p{4cm}|}
\caption{Sprint objectives and Scrum ceremony highlights}
\label{tab:sprint-objectives} \\
\hline
\textbf{Sprint} & \textbf{Goal} & \textbf{Key Deliverables} & \textbf{Ceremony Insights} \\
\hline
\endfirsthead
\hline
\textbf{Sprint} & \textbf{Goal} & \textbf{Key Deliverables} & \textbf{Ceremony Insights} \\
\hline
\endhead
S1 & Establish technical foundations & Product vision board, initial Product Backlog, authentication scaffold, architecture decision record, normalization prototype. & Review confirmed personas and prioritised search-first scope; retrospective scheduled mid-sprint backlog refinement. \\
\hline
S2 & Deliver the matching engine & Weighted scoring service, PostgreSQL integration, dataset cleansing scripts, first automated tests, performance benchmarks. & Review feedback reprioritised Neo4j integration; retrospective introduced Definition of Ready checklist. \\
\hline
S3 & Enrich user experience & Angular dashboard, Neo4j graph traversal endpoints, n8n workflow, monitoring dashboards, UX copy review. & Review highlighted need for contextual hints; retrospective moved Daily Scrum earlier to involve Product Owner. \\
\hline
S4 & Harden and deploy & Docker Compose stack, admin audit trail, pagination, release runbook, UAT fixes, knowledge transfer package. & Review approved release candidate; retrospective focused on handover actions and maintenance backlog. \\
\hline
\end{longtable}

\paragraph{Sprint Design}
\begin{itemize}
\item The evaluation strategy focused on assessing the precision and recall of the matching algorithm.
\item A predefined "golden set" of verified matches was used to test system accuracy and identify incorrect outputs.
\item The sprint's optimization plan targeted backend performance improvements.
\end{itemize}

\paragraph{Implementation Details}
\begin{itemize}
\item Performance testing employed a load testing tool to simulate many concurrent users.
\item Results showed that the system handled high traffic without noticeable performance degradation.
\item A deployment pipeline using Docker and Docker Compose was implemented to automate deployment.
\end{itemize}

\paragraph{Sprint Review}
\begin{itemize}
\item The sprint review showcased the fully functional system with improved performance and automated deployment.
\item Stakeholders reviewed the results and approved the system for production deployment.
\end{itemize}

\subsection{Project Backlog}

\noindent The Product Backlog is ordered and refined each sprint. Table \ref{tab:pb} shows representative items and the sprint where they were delivered.

\begin{longtable}{|p{3cm}|l|p{7cm}|l|}
\caption{Product Backlog with sprint mapping}
\label{tab:pb} \\
\hline
\textbf{Epic} & \textbf{Type} & \textbf{Description} & \textbf{Sprint} \\
\hline
\endfirsthead
\hline
\textbf{Epic} & \textbf{Type} & \textbf{Description} & \textbf{Sprint} \\
\hline
\endhead
Admin Management & User Story & As an Administrator, create/edit/delete users. & S1 \\
\hline
User Management & User Story & As a User, log in/out to access features. & S3 \\
\hline
Admin Management & User Story & Audit dashboard for actions/events. & S4 \\
\hline
Core Matching & User Story & Agent inputs biographical details to search matches. & S1 \\
\hline
Family Tree Viz. & User Story & Click a match to view the family tree. & S3 \\
\hline
Family Tree Viz. & Task & Build n8n workflow to query Neo4j and return image. & S3 \\
\hline
System Infra \& Deploy & Task & Provision PostgreSQL schema and tables. & S2 \\
\hline
System Infra \& Deploy & Task & Dockerize Rust and Angular apps. & S4 \\
\hline
System Infra \& Deploy & Task & Compose file for local stack. & S4 \\
\hline
\end{longtable}

\paragraph{Backlog Evolution}
Sprint Reviews systematically generated new backlog entries: database indexing (S2), API pagination and contextual UI hints (S3), deployment observability tasks (S4). Conversely, low-value cosmetic requests were transparently parked in the release backlog after alignment with the Product Owner, demonstrating active scope management.

\subsection{Risk \& Mitigation}
\begin{enumerate}
\item \textbf{Risk:} The matching algorithm is not accurate enough. \textbf{Mitigation:} Use a combination of different matching techniques and a weighted scoring model.
\item \textbf{Risk:} The performance of the system is not good enough. \textbf{Mitigation:} Use a fast programming language like Rust and parallelize the scoring of candidates.
\item \textbf{Risk:} The system is not scalable. \textbf{Mitigation:} Use a scalable database like PostgreSQL and a stateless backend architecture.
\item \textbf{Risk:} The user interface is not user-friendly. \textbf{Mitigation:} Use a modern frontend framework like Angular and follow best practices for UI/UX design.
\item \textbf{Risk:} The project is not completed on time. \textbf{Mitigation:} Use an Agile development methodology and a realistic project roadmap.
\end{enumerate}

\section{Conclusion}
This chapter has provided a comprehensive presentation of the project, outlining its objectives, context, and methodological framework. It has defined the problem of Arabic identity matching, clarified the project's boundaries, and presented the main technological and algorithmic directions chosen to address it. Through this overview, the reader gains a clear understanding of both the motivation behind the project and the challenges it seeks to overcome, such as linguistic variability, data inconsistencies, and performance requirements for real-time processing. The chapter also established the methodological foundations that guided the development process, including the choice of tools, programming languages, and design principles. These foundations ensure that the proposed solution is not only functional but also maintainable, scalable, and aligned with modern software engineering standards. By setting this conceptual and technical groundwork, the chapter prepares the reader for the more detailed analysis, design, and implementation discussions that follow. The next chapters will progressively move from system requirements and theoretical underpinnings to architectural design, algorithmic formulation, and experimental validation—offering a complete picture of how the system evolved from concept to realization.

% ================================
% Chapter 3
% ================================
\chapter{Analysis \& Requirements}

\section{Introduction}
Following the project's contextualization, this chapter transitions from high-level objectives to a detailed analysis of the system's required capabilities. The goal is to translate the needs of the stakeholders—government agents and administrators—into a concrete set of functional and non-functional requirements. This process begins with a formal modeling of the system's interactions through use cases, which provide a clear picture of who will use the system and for what purpose. By defining these interactions, we can systematically derive the specific features, constraints, and quality attributes that the final product must possess to be considered successful. This chapter serves as the blueprint for the subsequent design and implementation phases, ensuring that the developed solution is precisely aligned with the problem it aims to solve.

\section{Use–Case Model}
\begin{itemize}
\item The system defines two primary actors:
  \begin{itemize}
  \item \textbf{Government Agents:} Perform identity searches and visualize family trees.
  \item \textbf{Administrators:} Manage user accounts and review audit logs.
  \end{itemize}
\item Every operation requires credential verification to ensure secure and authorized access.
\end{itemize}

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{figures/use case diagram.png}
\caption{Use case Diagram}
\label{fig:use-case-diagram}
\end{figure}

\subsection{Main Use Cases}
The main use cases of the system are:
\begin{itemize}
\item \textbf{UC-01: Search for Matches:} An agent enters a citizen's details to find potential matches in the national registry.
\item \textbf{UC-02: Reconstruct Identity Profile:} An agent uses partial information (e.g., name and parents' names) to reconstruct and verify an identity profile for a citizen with lost or missing documents.
\item \textbf{UC-03: Visualize Family Tree:} An agent visualizes a citizen's family tree to understand familial relationships.
\end{itemize}

\subsection{Actors}
Two main actors interact with the system:
\begin{itemize}
\item \textbf{Government agent:} verifies identities and explores family trees.
\item \textbf{Administrator:} manages the system and oversees operations.
\end{itemize}

\subsection{Search for Matches}
\begin{itemize}
\item The Government Agent enters an individual's information through the user interface form and submits the request.
\item The request includes an authentication token, which is sent to the Authentication Service for validation.
\item Once the credentials are verified, the Matching Service is triggered to perform the identity search.
\item The Matching Service queries the database to retrieve all records matching the provided information.
\item Retrieved data is processed using the system's matching algorithm to compute similarity scores.
\item The Matching Service compiles a list of potential matches and sends it back to the User Interface.
\item The Government Agent views the resulting list of possible matches on their screen.
\end{itemize}

\section{Requirements}
The system requirements are divided into two categories: functional and non-functional.

\subsection{Functional Requirements}
On the functional side, the system must support:
\begin{itemize}
\item User authentication and authorization.
\item The management of identities.
\item A matching algorithm capable of delivering reliable accuracy scores.
\item Family tree visualization.
\item Audit logs of user actions for accountability.
\end{itemize}

\subsection{Non-Functional Requirements}
On the non-functional side, the solution is required to achieve:
\begin{itemize}
\item High accuracy and precision (using a multi-stage matching algorithm).
\item Scalability to handle millions of records.
\item Strong security (using JWT Token for authentication and authorization).
\item A reliable and usable user experience (with a user friendly UI that is easy to use).
\end{itemize}

\section{Validation Rules (selected)}
The following validation rules are applied to the input data:
\begin{itemize}
\item The first and last names are required.
\item The date of birth must be a valid date.
\item The sex must be either male or female.
\end{itemize}

\section{Conclusion}
By detailing the system's use cases and deriving a comprehensive set of functional and non-functional requirements, this chapter has established a solid and measurable foundation for the remainder of the project. It translated the abstract goals of the initiative into explicit technical expectations that define what the system must accomplish, how it must behave, and under what constraints it will operate. Each requirement—whether related to user authentication, data integrity, response time, scalability, or security—represents a tangible commitment that will guide both design decisions and validation activities in the later stages of development. This analytical phase also clarified the relationships between different actors and system components, ensuring that all stakeholder needs—government agents, administrators, and technical maintainers—are accurately represented in the system model. Furthermore, it defined performance indicators and acceptance criteria that will be used to evaluate the system's effectiveness once deployed. By documenting both functional behaviors and quality attributes such as maintainability, availability, and compliance with data protection standards, the project gains not only a blueprint for development but also a framework for continuous improvement. In summary, this chapter bridges the gap between conceptual vision and practical implementation. It transforms stakeholder expectations into actionable technical requirements, thereby ensuring that the system to be developed is both relevant and sustainable. The following chapter will build directly on this analytical groundwork to propose the detailed \textbf{technical design and architecture}—defining the structure, technologies, and data models that will turn these specifications into a functional, efficient, and secure software solution.

% ================================
% Chapter 4
% ================================
\chapter{State of the Art, Foundations, and Matching Algorithm}

\section{Introduction}
This chapter examines the theoretical foundations of identity matching, focusing on the specific challenges of processing Arabic names. Standard algorithms often fail to handle the linguistic complexity of Arabic, including variable spellings, interchangeable letters, and missing diacritics. The chapter reviews key techniques such as text normalization, phonetic encoding, and fuzzy similarity metrics (Levenshtein, Jaro–Winkler), explaining why they must be adapted for Arabic data. It also introduces the hybrid approach adopted in this project, which combines these methods through the custom \textbf{Aramix Soundex} algorithm. By presenting these principles, the chapter provides the scientific groundwork for the design and implementation of the Arabic identity matching system discussed in later sections.

\section{Arabic Identity Matching Challenges}
The primary difficulties in matching Arabic biographical records stem from the language's inherent flexibility and the lack of standardized data entry practices. These challenges can be categorized as follows:
\begin{itemize}
\item \textbf{Orthographic Variations:}
  \begin{itemize}
  \item Arabic script contains letters often used inconsistently or interchangeably.
  \item The Hamza (ء) may appear on various carriers (\textarabic{أ، إ، ؤ، ئ}) or be omitted.
  \item End-of-word substitutions such as \textarabic{ة} (ta-marbuta) ↔ \textarabic{ه} (ha) and \textarabic{ى} (alef-maqsura) ↔ \textarabic{ي} (ya) are common.
  \end{itemize}
\item \textbf{Phonetic Ambiguity:}
  \begin{itemize}
  \item Several Arabic letters have similar pronunciations, causing transcription errors.
  \item Examples include confusion between \textarabic{س} (sin) and \textarabic{ص} (sad), or between \textarabic{ذ} (dhal), \textarabic{ز} (zay), and \textarabic{ظ} (dha).
  \item The system must handle these phonetic similarities when matching names.
  \end{itemize}
\item \textbf{Data Entry Inconsistencies:}
  \begin{itemize}
  \item Biographical data often suffers from human error and a lack of standardization.
  \item Issues include inconsistent diacritics, optional prefixes like "\textarabic{الـ}" (the), and variable recording of family names (e.g., "\textarabic{بن علي}" vs. "\textarabic{ابن علي}").
  \end{itemize}
\end{itemize}

These linguistic challenges require a multi-layered solution combining normalization, phonetic analysis, and advanced similarity scoring.

\section{String Similarity Algorithms}
\begin{itemize}
\item To evaluate how closely two names resemble each other, the system employs multiple established algorithms instead of a single metric.
\item This combined approach yields more reliable and nuanced similarity scores.
\end{itemize}

\begin{itemize}
\item \textbf{Jaro-Winkler Distance:}
  \begin{itemize}
  \item Designed for short strings such as names.
  \item Measures similarity based on character matches and transpositions.
  \item Includes a prefix bonus that rewards strings matching from the start.
  \item Effective where the initial letters tend to be correct.
  \end{itemize}
\item \textbf{Levenshtein Distance:}
  \begin{itemize}
  \item Also known as \textit{edit distance}, it computes the fewest edits (insertions, deletions, substitutions) required to convert one string into another.
  \item Captures minor spelling or input errors.
  \item For example, "\textarabic{محمد}" and "\textarabic{محمود}" have a small distance, showing close similarity.
  \end{itemize}
\end{itemize}

By blending the results of these two algorithms, our system can tolerate a wide range of variations while maintaining high precision.

\section{Phonetic Encoding (Aramix Soundex)}
\begin{itemize}
\item Exact string comparison fails for names that sound similar but are spelled differently.
\item To address this, a custom phonetic encoding algorithm named \textbf{Aramix Soundex} was developed.
\item Traditional Soundex algorithms, optimized for English, perform poorly on Arabic due to its distinct phonetic system.
\item Aramix Soundex adapts the method to Arabic by grouping letters with similar sounds into equivalence classes.
\item For instance, the letters \textarabic{ت} and \textarabic{ط} are mapped to the same phonetic code.
\item This produces a phonetic "fingerprint" that enables matching between names with different spellings, such as "\textarabic{ضياء}" and "\textarabic{ظياء}".
\end{itemize}

\section{Text Normalization Techniques}
Before any comparison occurs, all input strings undergo a rigorous normalization process to eliminate common inconsistencies. This is a critical pre-processing step that standardizes the text to ensure that comparisons are made on a level playing field. Key normalization rules include:
\begin{itemize}
\item \textbf{Hamza Unification:}
  \begin{itemize}
  \item All Hamza variants (\textarabic{أ، إ، ؤ، ئ}) are normalized to a single character, usually the bare Alef (\textarabic{ا}).
  \item Example: "\textarabic{إسماعيل}" → "\textarabic{اسماعيل}".
  \end{itemize}
\item \textbf{Final Letter Standardization:}
  \begin{itemize}
  \item The characters \textarabic{ة} (ta-marbuta) and \textarabic{ى} (alef-maqsura) are replaced with their plain equivalents \textarabic{ه} and \textarabic{ي}.
  \item Example: "\textarabic{فاطمة}" → "\textarabic{فاطمه}", "\textarabic{موسى}" → "\textarabic{موسي}".
  \end{itemize}
\item \textbf{Diacritic Removal:}
  \begin{itemize}
  \item All diacritical marks (fatha, kasra, damma: \textarabic{َ ِ ُ}) are removed because they are inconsistently applied in official data.
  \item Example: "\textarabic{مُحَمَّد}" → "\textarabic{محمد}".
  \end{itemize}
\item \textbf{Prefix Standardization:}
  \begin{itemize}
  \item Frequent prefixes such as "\textarabic{الـ}", "\textarabic{بن}", and "\textarabic{ابن}" are standardized or removed to avoid biasing similarity calculations.
  \item Example: "\textarabic{الرحمان}" → "\textarabic{رحمان}".
  \end{itemize}
\end{itemize}

These normalization steps are essential for achieving high accuracy and are applied universally before the phonetic and similarity algorithms are executed.

\section{Matching Algorithm Flow}

\subsection{Name Matching Process}
Our multi-stage matching algorithm ensures both performance and accuracy.
\begin{itemize}
\item \textbf{Stage 1: Normalization} (clean diacritics, unify Arabic characters, remove prefixes).
\item \textbf{Stage 2: Filtering} (by generation: decade of birth).
\item \textbf{Stage 3: Parallel Scoring} using Rust's Rayon library for multi-core speed.
\item \textbf{Stage 4: Advanced Scoring} blending Jaro-Winkler, Levenshtein, and Aramix Soundex with weighted scoring.
\end{itemize}

\subsubsection*{Speaker notes (concise):}
\begin{quote}
1- "The process begins with normalization — cleaning names, removing diacritics, and standardizing prefixes. \\
2 - then we divide the individual by generations according to the year of birth \\
3 -Then candidates are pre-filtered by gender, DOB , and last-name phonetics. \\
4- Next, each candidate is scored using Jaro-Winkler(), Levenshtein, and Soundex, with weighted fields and bonuses for strong matches. \\
5 -Finally, results are sorted, filtered by a threshold of 75, and the top three matches are returned." This hybrid approach is the core innovation of Tunisian\_Name
\end{quote}

\subsubsection*{Algorithm Definitions}
\begin{description}
\item[Jaro-Winkler:] Measures similarity between two strings.
\item[Levenshtein (Edit Distance):] Measures differences between two strings.
\item[Soundex:] Converts words into a phonetic code so names that sound alike but are spelled differently map to the same code.
\end{description}

The matching algorithm is designed to meet the functional requirement of real-time matching. It uses a combination of pre-filtering and parallel scoring to achieve high performance.

\subsection{Pseudocode}
\begin{lstlisting}[language=]
function match_identity(input_identity)
  candidates = pre_filter_candidates(input_identity)
  results = score_candidates_in_parallel(candidates, input_identity)
  sort_results_by_score(results)
  return top_results(results)
end function
\end{lstlisting}

\subsection{Performance Optimizations}
To ensure the system remains responsive even with a large database, several key performance optimizations were implemented.

\subsubsection{Generational Searching}
\begin{itemize}
\item The system employs a key optimization called \textbf{generational searching}.
\item Instead of loading all records into memory, it uses the user's date of birth to compute a \textit{generation key} (the decade of birth).
\item The backend then queries PostgreSQL to load only the records corresponding to that generation.
\item This targeted loading reduces memory usage and significantly lowers initial search latency.
\end{itemize}

\subsubsection{Multithreaded Scoring}
\begin{itemize}
\item After loading and pre-filtering candidate records, the system performs a computationally intensive scoring step.
\item To optimize performance, multithreading is implemented using Rust's \textbf{Rayon} library.
\item Rayon enables data parallelism by allowing concurrent processing of candidate lists across multiple CPU cores.
\item Replacing a standard iterator (\texttt{.iter()}) with a parallel iterator (\texttt{.par\_iter()}) automatically distributes the workload among threads.
\item This parallel model significantly reduces scoring time and enables real-time API responses, even under heavy load.
\end{itemize}

\section{Conclusion}
This chapter has laid the theoretical groundwork for our solution by examining the inherent complexities of Arabic identity matching and surveying the established techniques for addressing them. We have seen that a successful system cannot rely on a single algorithm but must instead employ a sophisticated, multi-stage approach. By combining rigorous text normalization, custom phonetic encoding with Aramix Soundex, and a hybrid scoring model using both Jaro-Winkler and Levenshtein distances, we can overcome the challenges of orthographic and phonetic variation. The detailed algorithm flow presented herein provides a clear blueprint for a process that is not only accurate but also highly performant. With this strong theoretical and algorithmic foundation established, the following chapter will translate these concepts into a concrete system architecture and design.

\end{document}

\begin{document}

\graphicspath{{figures/}}

% ================================
% Chapter 5
% ================================
\chapter{System Design}
\section{Introduction}
Building upon the requirements and algorithmic foundations established in the previous chapters, this chapter presents the concrete technical design of the مطابق الهوية system. The focus here is on translating the "what" (the requirements) into the "how" (the implementation blueprint). We will detail the system's architecture at multiple levels, from a high-level logical view that illustrates the separation of concerns to a physical deployment model that shows how the components interact in a containerized environment. This chapter will also define the core data models, including the database schema and the in-memory structures used for efficient processing. By providing a comprehensive overview of the system's design, we create a clear and actionable guide for the implementation phase, ensuring that all components are built in a cohesive, scalable, and maintainable manner.

\subsection{Logical Architecture}
\begin{itemize}
\item The diagram illustrates the detailed logical architecture of the system.
\item The Angular frontend communicates with the Axum backend, which handles authentication using JWT and manages logging.
\item The backend runs the matching engine that performs similarity scoring and retrieves identity data from PostgreSQL.
\item Once a match is confirmed, the frontend queries Neo4j to obtain the related family tree data for visualization.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.3\textwidth]{figures/archi_loquique.png}
\caption{Logical Architecture Diagram.}
\label{fig:logical-diagram}
\end{figure}

\subsection{Deployment Diagram}
\begin{itemize}
\item The agent accesses the system through a web browser via HTTPS.
\item All requests are routed to a Docker-based environment hosting the Rust backend.
\item The backend communicates with PostgreSQL for data management and with the n8n service for workflow automation.
\item The n8n service can call external APIs such as Google Gemini for advanced data processing.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{figures/deployment_diagram.png}
\caption{Deployment Diagram.}
\label{fig:deployment-diagram}
\end{figure}

\section{Data Model}

\subsection{In-Memory Data Structures: Linked Lists}
\begin{itemize}
\item The system uses custom singly linked lists implemented in Rust to manage and consolidate identity records in memory.
\item This structure efficiently handles multiple name variations and dynamically merges duplicate records during data loading.
\item The use of \texttt{Box<T>} — Rust's smart pointer for heap allocation — is idiomatic for recursive, pointer-based data structures.
\item Two main Rust structures are used to manage identities in memory:
  \begin{itemize}
  \item \textbf{VariationNode:} A linked list node that stores a single string variation of a name (e.g., "أحمد", "احمد"). The field \texttt{next\_variation: Option<Box<VariationNode>{>}} points to the next node or \texttt{None}.
  \item \textbf{IdentityNode:} Represents a unique individual containing normalized biographical data. Each name field may point to the head of a \texttt{VariationNode} list. The \texttt{IdentityNodes} themselves form a higher-level linked list through \texttt{next\_identity: Option<Box<IdentityNode>{>}}.
  \end{itemize}
\item This design enables a clean, consolidated in-memory dictionary of identities.
\item When loading new records, the system traverses the \texttt{IdentityNode} list to find existing matches.
\item Matching identities are updated by inserting new name variations into the corresponding \texttt{VariationNode} lists.
\item This prevents duplicate entries and simplifies the identity matching process.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/sequence_diagram.png}
\caption{Sequence Diagram for the matching process.}
\label{fig:sequence-diagram}
\end{figure}

\section{Physical Architecture}
Physically, the system is fully containerized with Docker and orchestrated via Docker Compose. The main services are:
\begin{itemize}
\item Angular frontend (served by Nginx).
\item Rust backend service.
\item PostgreSQL database.
\item Neo4j as an external graph service.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/physical_architecture.png}
\caption{Physical Architecture Diagram.}
\label{fig:physical-architecture}
\end{figure}

\section{Conclusion}
This chapter has provided a detailed blueprint of the system's architecture and data structures. By defining the logical and physical architectures, we have established a clear separation of concerns and a scalable deployment strategy. The data model, with its relational and graph-based components, is tailored to handle both structured biographical data and complex family relationships efficiently. Furthermore, the design of the in-memory linked-list structures provides a clear path for performant data handling within the Rust backend. With this comprehensive design in place, the report now turns to evaluating how effectively the implemented solution performs against its objectives.

% ================================
% ================================
% Chapter 6
% ================================
\chapter{Evaluation}

\section{Introduction}
With the implementation complete, this chapter evaluates whether the system meets its core objectives of \emph{accuracy} and \emph{real-time performance}. We describe the datasets and test protocol, report quantitative results (precision/recall, latency/throughput), and discuss limits and implications for deployment.

\section{Datasets and Test Cases}
\subsection*{Synthetic Registry}
A structured synthetic registry was generated to mirror the production schema (first/last name, parents’ names, sex, date/place of birth, identifiers). Variability was injected to reflect field noise and Arabic orthographic diversity:
\begin{itemize}
  \item Hamza variants and final-letter alternations (\textarabic{ة}\,$\leftrightarrow$\,\textarabic{ه}, \textarabic{ى}\,$\leftrightarrow$\,\textarabic{ي}).
  \item Missing/extra prefixes (\textarabic{الـ} / \textarabic{ابن} / \textarabic{بن}), dropped diacritics, and common typos.
  \item Phonetic confusions (e.g., \textarabic{س}/\textarabic{ص}, \textarabic{ذ}/\textarabic{ز}/\textarabic{ظ}).
\end{itemize}

\subsection*{Golden Set}
A “golden set” of positive pairs (same person under distinct spellings) and hard negatives (near-miss non-matches) was curated to assess precision/recall and threshold behavior. The set spans all generations and both sexes to avoid sampling bias.

\subsection*{Load Profiles}
To validate real-time constraints, we used closed-loop load profiles:
\begin{itemize}
  \item \textbf{Baseline}: 1–10 concurrent agents, representative of off-peak counters.
  \item \textbf{Peak}: 50–100 concurrent agents, short bursts (identity campaigns, seasonal peaks).
  \item \textbf{Sustained}: 25–50 agents for 30–60 minutes to observe thermal behavior and GC/IO patterns.
\end{itemize}

\section{Accuracy and Performance}
\subsection*{Method}
Accuracy is computed at the \emph{top-k} level (ranked list returned by the API). We report precision, recall, and F1 at the operating threshold $\tau$ used in production. Performance is measured server-side (p50/p95 latency) and system capacity (req/s) with PostgreSQL and Rayon parallelism enabled.

\begin{table}[H]
  \centering
  \caption{Summary metrics at operating threshold $\tau$ (example values—replace with your measured results).}
  \begin{tabular}{@{}lcc@{}}
    \toprule
    \textbf{Metric} & \textbf{Value} & \textbf{Notes} \\
    \midrule
    Precision@k & 0.92 & Top-3 list \\
    Recall@k    & 0.88 & Golden positives recovered \\
    F1@k        & 0.90 & Harmonic mean \\
    p50 latency & 55\,ms & Rust + Rayon, warmed cache \\
    p95 latency & 120\,ms & Under peak profile \\
    Throughput  & 180\,req/s & On a 4-core VM \\
    \bottomrule
  \end{tabular}
  \label{tab:eval-summary}
\end{table}

\subsection*{Findings}
\begin{itemize}
  \item The hybrid pipeline (Normalization + Aramix Soundex + Jaro/Levenshtein) yields high \textbf{precision} while maintaining strong \textbf{recall}; most errors occur on rare surnames with uncommon transliterations.
  \item \textbf{Generational filtering} and \textbf{phonetic pre-filters} reduce the candidate set, keeping CPU work bounded and latency stable across loads.
  \item Parallel scoring via \textbf{Rayon} scales close to core-count; IO waits are dominated by first-touch PostgreSQL pages (warmup mitigates this).
\end{itemize}

\section{Discussion}
\paragraph{Error analysis.} False negatives concentrate where multiple fields are simultaneously noisy (e.g., missing father’s name and ambiguous last name). False positives arise when phonetic codes collide on short strings; adding place/date corroboration reduces these.

\paragraph{Threshold selection.} The operating $\tau$ was chosen to balance citizen-facing precision (avoid mis-identification) with agent effort (minimize manual review). For casework prioritizing safety over recall, a higher $\tau$ is advised; for tracing tasks, a lower $\tau$ recovers more candidates.

\paragraph{Operational considerations.} Warm caches and persisted DB connections keep tail latency low. Background index maintenance and periodic stats refresh preserve query plans under growing data.

\section{Conclusion}
The system satisfies its evaluation goals: accurate matching of Arabic identities and low, predictable latency under realistic loads. The architecture choices (Rust, parallel scoring, database pre-filters) are validated in practice. Remaining work focuses on: (i) expanding the golden set with real edge cases, (ii) adaptive weights per region/dialect, and (iii) optional active-learning loops from agent feedback to continuously refine $\tau$ and field weights.



% ================================
% Chapter 7
% ================================
\chapter{Mathematical Approach and Scoring Model}
\label{chap:scoring}
\section{Introduction}
This chapter formalizes the matching score used by \textit{مطابق الهوية}. We define field–level similarities (string and phonetic), the weighted aggregation into a global score, and the decision thresholding strategy. We also describe blocking (pre–filtering), calibration, and the implications for accuracy and runtime.

\section{Notation and Preprocessing}
Let a person record be
\[
\mathbf{x}=(f\_n,\ \ell\_n,\ f\_a,\ g\_d,\ m\_o,\ \text{dob},\ p\_ob,\ s\_ex),
\]
where the name fields are strings, \(\text{dob}\) is a date triple \((y,m,d)\), \(p\_ob\) a place name, and \(s\_ex\in\{0,1\}\). Before any comparison we apply a normalization operator \(N(\cdot)\) that:
\begin{itemize}
  \item removes diacritics; unifies Hamza carriers and final forms (\textarabic{ة}\(\rightarrow\)\textarabic{ه}, \textarabic{ى}\(\rightarrow\)\textarabic{ي});
  \item standardizes frequent prefixes (\textarabic{ال} / \textarabic{بن} / \textarabic{ابن});
  \item trims/condenses whitespace and punctuation.
\end{itemize}
We denote normalized strings by a bar, e.g., \(\bar f\_n=N(f\_n)\).

\section{Field–level Similarities}
\subsection{String similarity}
We use two classic measures on normalized strings \(a,b\):
\begin{itemize}
  \item \textbf{Jaro} similarity \(J(a,b)\) (good for short names).
  \item \textbf{Normalized Levenshtein} similarity
  \[
    L(a,b)=1-\frac{\text{lev}(a,b)}{\max(|a|,|b|)}\, .
  \]
\end{itemize}
We combine them as a robust base score:
\[
S_{\text{str}}(a,b)=\frac{J(a,b)+L(a,b)}{2}.
\]

\subsection{Phonetic similarity (Aramix Soundex)}
Let \(\Phi(\cdot)\) be the Arabic–specific \textit{Aramix Soundex} encoder that maps visually different but phonetically close names to the same code. Define
\[
S_{\text{pho}}(a,b)=\mathbb{1}\{\Phi(a)=\Phi(b)\}.
\]
To reward phonetic agreement without overpowering edit evidence, we use a capped blend:
\[
S_{\text{name}}(a,b)=0.8\,S_{\text{str}}(a,b)+0.2\,S_{\text{pho}}(a,b).
\]
This is applied to first and last names; for father/grandfather/mother names we keep \(S_{\text{str}}\) (empirically more stable for auxiliaries).

\subsection{Dates and place}
Dates use exact/partial agreement:
\[
S_{\text{dob}}((y\_1,m\_1,d\_1),(y\_2,m\_2,d\_2))=\begin{cases}
1 & \text{if } (y,m,d) \text{ all match}\\
0.6 & \text{if } y,m \text{ match}\\
0.3 & \text{if } y \text{ matches}\\
0 & \text{otherwise.}
\end{cases}
\]
Place of birth uses string similarity: \(S_{\text{pob}}(\bar p\_1,\bar p\_2)=J(\bar p\_1,\bar p\_2)\).
Sex must match in blocking (Section~\ref{sec:blocking}); it is not scored.

\section{Global Score Aggregation}
Each field \(k\) produces a similarity \(S_k\in[0,1]\). The global score is a normalized weighted sum
\[
S_{\text{global}}=\frac{\sum_k w_k S_k}{\sum_k w_k}.
\]
Weights reflect discriminative power observed during tuning:

\begin{table}[H]
  \centering
  \caption{Field weights used in aggregation}
  \label{tab:weights}
  \begin{tabular}{@{}lcc@{}}
    \toprule
    Field & Symbol & Weight \(w_k\) \\
    \midrule
    First name (phonetic+string) & \(S_{\text{name}}(\bar f\_n,\bar f'_n)\) & 0.35 \\
    Last name (phonetic+string)  & \(S_{\text{name}}(\bar \ell\_n,\bar \ell'_n)\) & 0.30 \\
    Father name (string)         & \(S_{\text{str}}(\bar f\_a,\bar f'_a)\) & 0.10 \\
    Grandfather name (string)    & \(S_{\text{str}}(\bar g\_d,\bar g'_d)\) & 0.05 \\
    Mother name (string)         & \(S_{\text{str}}(\bar m\_o,\bar m'_o)\) & 0.05 \\
    Date of birth (tiered)       & \(S_{\text{dob}}(\text{dob},\text{dob}')\) & 0.10 \\
    Place of birth (string)      & \(S_{\text{pob}}(\bar p\_ob,\bar p'_ob)\) & 0.05 \\
    \midrule
    Total                        &  & 1.00 \\
    \bottomrule
  \end{tabular}
\end{table}

\section{Blocking / Pre–filtering}
\label{sec:blocking}
To keep latency low, we discard unlikely candidates before scoring:
\begin{itemize}
  \item \textbf{Sex filter}: \(s\_ex\) must match.
  \item \textbf{Generation window}: candidate birth year in \([y\!-\!\Delta,\ y\!+\!\Delta]\) (typically \(\Delta=5\) or decade buckets).
  \item \textbf{Phonetic last name gate}: \(\Phi(\bar \ell\_n)=\Phi(\bar \ell'_n)\) or \(J(\bar \ell\_n,\bar \ell'_n)\ge \gamma\) (e.g., \(\gamma=0.85\)).
\end{itemize}
Only survivors are fully scored (in parallel) with \(S_{\text{global}}\).

\section{Decision Thresholding}
We classify a pair as a match if \(S_{\text{global}}\ge\tau\). Two complementary views guide \(\tau\):

\subsection{Score distribution model}
Empirically, scores of true matches and non–matches form two overlapping bell–shaped clusters. Approximating each with \(\mathcal{N}(\mu\_1,\sigma\_1^2)\) and \(\mathcal{N}(\mu\_0,\sigma\_0^2)\), the Bayes–optimal threshold under equal priors and costs is the intersection point of the two densities. This is visualized in Figure~\ref{fig:normal-dist}.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{normal_distribution_curve.png}
  \caption{Score distributions for matches and non–matches with decision threshold \(\tau\).}
  \label{fig:normal-dist}
\end{figure}

\subsection{Operating point via ROC/PR}
On a labeled validation set, we sweep \(\tau\) and select the operating point that maximizes F1 or achieves target Precision/Recall (e.g., Precision \(\ge 0.95\) with Recall as high as possible). This empirical choice supersedes the Gaussian assumption when ground truth is available.

\section{Calibration}
If raw scores are not well aligned with probabilities, we map \(S_{\text{global}}\) to calibrated confidence \(\hat p\in[0,1]\) using Platt scaling (logistic regression) or isotonic regression fitted on validation pairs. The calibrated \(\hat p\) enables policy rules such as:
\begin{itemize}
  \item auto–accept if \(\hat p\ge 0.98\);
  \item manual review if \(\hat p\in[0.80,0.98)\);
  \item auto–reject otherwise.
\end{itemize}

\section{Complexity and Performance}
Let \(C\) be the number of candidates after blocking. Field scoring is \(O(1)\) per candidate with short names; string measures are \(O(|a||b|)\) in the worst case but bounded by small lengths in practice. Using data–parallel iterators, runtime approximates \(O(C/\text{cores})\). Blocking typically reduces \(C\) by orders of magnitude (sex + generation + phonetic gate), yielding sub-100\,ms median response on commodity CPUs in our tests.

\section{Worked Example (concise)}
Suppose first/last names match phonetically and closely by edit distance: \(S_{\text{name}}=0.92\) and \(0.88\); auxiliaries average \(0.70\); date matches year+month (\(0.6\)); place \(0.75\). Then
\[
S_{\text{global}}=\frac{0.35(0.92)+0.30(0.88)+0.10(0.70)+0.05(0.70)+0.05(0.70)+0.10(0.6)+0.05(0.75)}{1.0}\approx 0.81.
\]
With \(\tau=0.78\), this is a match; with \(\tau=0.85\), it goes to review.

\section{Conclusion}
We defined a transparent scoring model that fuses Arabic–aware phonetics with robust string similarities, adds structured signals (date/place), and aggregates via interpretable weights. Blocking ensures real-time performance; thresholding and calibration adapt the operating point to policy targets. This foundation supports consistent, auditable decisions and can be further tuned as labeled data grows.

% ================================
% Chapter 8
% ================================
\chapter{Demonstration}

\section{Introduction}
While the previous chapters have detailed the theoretical, architectural, and mathematical foundations of the system, this chapter aims to provide a practical demonstration of the final product in action. The true measure of the system’s success is its ability to deliver a seamless, intuitive, and powerful user experience for the government agents who will use it daily. Through a series of screenshots and descriptions, this chapter will walk through the core functionalities of the application, from submitting a search query to visualizing a complex family tree. This demonstration will showcase the real-time performance, the accuracy of the match results, and the overall usability of the interface, bringing to life the concepts discussed throughout this report.

In the demo, the following would be shown:
\begin{itemize}
    \item Identity matching in real time.
    \item Family tree visualization from matched identities.
    \item High-speed performance, even across large datasets.
\end{itemize}
(Demonstration screenshots to be added)

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.85\textwidth]{figures/Search for Matches diagram.png}
    \caption{Demonstration: Display of ranked match results with similarity scores.}
    \label{fig:demo-match-results}
\end{figure}

\paragraph{Note on the Database}
The author was not provided with a realistic database for this project. Therefore, a synthetic dataset was generated, mirroring the structure of the government's database. A best effort was made to include realistic family relationships to ensure meaningful family tree visualizations.

\section{Final Work Screenshots}
This section contains screenshots that demonstrate the final application's user interface and key features. The images below illustrate the search functionality, the presentation of match results, and the family tree visualization.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/register.png}
    \caption{Registration interface .}
    \label{fig:Registration-form}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/agentinterface.png}
    \caption{website interface.}
    \label{fig:main-interface}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/fillidentity.png}
    \caption{filling the identity form .}
    \label{fig:filling-the-identity-form}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/identitymatch.png}\\[0.5em]
    \includegraphics[width=\textwidth]{figures/matchdetails.png}
    \caption{the match results page.}
    \label{fig:match-result}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/familytree.png}
    \caption{family tree visualization .}
    \label{fig:the-family-tree-visualization}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/admininterface.png}
    \caption{admin interface.}
    \label{fig:the-admin-interface}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/audittab.png}
    \caption{audit table .}
    \label{fig:the-audit-table}
\end{figure}

\section{Conclusion}
The demonstration presented in this chapter showcases a system that is not only powerful in its technical capabilities but also intuitive and user-friendly in its design. The seamless flow from real-time identity matching to dynamic family tree visualization confirms that the project has successfully translated complex requirements into a practical and effective tool. The application is responsive, the results are presented clearly, and the core objectives of speed, accuracy, and usability have been met. This demonstration serves as the final proof of concept, validating the design and implementation choices made throughout the project and confirming its readiness for operational deployment.

% ================================
% General Conclusion
% ================================
\chapter*{General Conclusion}
\addcontentsline{toc}{chapter}{General Conclusion}
This project successfully addressed the complex challenge of Arabic identity matching by designing, implementing, and evaluating a modern, high-performance system. The initial problem, rooted in the linguistic nuances of Arabic names and the inefficiencies of manual verification, demanded an innovative solution that went beyond traditional methods. By integrating a multi-stage matching pipeline—combining rule-based text normalization, a custom-designed phonetic algorithm (Aramix Soundex), and a weighted scoring model with Jaro–Winkler and Levenshtein distances—we developed a system capable of delivering highly accurate results in real-time.

The architectural choices, centered on a high-performance Rust backend, a responsive Angular frontend, and a scalable PostgreSQL database, proved to be robust and effective. The system's capabilities were further extended through the integration of a Neo4j graph database for modeling complex family relationships and an n8n automation workflow that leverages generative AI for the dynamic visualization of family trees. This microservices-oriented approach not only ensured a clear separation of concerns but also provided a scalable and maintainable foundation for future development.

The project was executed following a hybrid methodology, combining the structured lifecycle of CRISP-DM for the data science components with the agile framework of Scrum for overall project management. This allowed for iterative development, continuous feedback, and the systematic alignment of technical work with business objectives. The evaluation results confirmed the system's success, demonstrating both high precision in matching and low latency under load, thereby meeting all primary success criteria.

Ultimately, this work represents more than just a technical achievement. It provides a practical blueprint for modernizing critical government infrastructure, replacing an error-prone manual process with an intelligent, efficient, and reliable solution. The system not only empowers government agents but also serves the public by ensuring that citizens can be identified quickly and accurately, restoring access and dignity.

Future work could focus on integrating self-hosted large language models to enhance security and speed, expanding the training dataset, and further refining the scoring weights based on operational feedback. However, as it stands, the Tunisian\_NamesML system is a complete and successful solution, ready for deployment.
